{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c6f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6d37d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Preprocessing ===\n",
    "df = pd.read_csv('../data/car_data.csv')\n",
    "df4=pd.read_csv('../data/car_data4.csv')\n",
    "df = pd.concat([df4, df], ignore_index=True)\n",
    "df['model'] = df['name'].apply(lambda x: ' '.join(x.split()[1:]))\n",
    "df['car_age'] = 2025 - df['year']\n",
    "df.drop(['Unnamed: 0', 'name', 'year'], axis=1, inplace=True)\n",
    "df['Price'] = np.log1p(df['Price'])\n",
    "\n",
    "for col in ['company', 'fuel_type', 'model']:\n",
    "    df[col] = df[col].astype('category')\n",
    "    df[col + '_cat'] = df[col].cat.codes\n",
    "\n",
    "cat_cols = ['company_cat', 'fuel_type_cat', 'model_cat']\n",
    "num_cols = ['car_age', 'kms_driven']\n",
    "target_col = 'Price'\n",
    "\n",
    "X_categorical = torch.tensor(df[cat_cols].values, dtype=torch.long)\n",
    "scaler = StandardScaler()\n",
    "X_numerical = scaler.fit_transform(df[num_cols])\n",
    "X_numerical = torch.tensor(X_numerical, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(df[target_col].values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a33981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Split ===\n",
    "X_cat_train, X_cat_val, X_num_train, X_num_val, y_train, y_val = train_test_split(\n",
    "    X_categorical, X_numerical, y_tensor, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = TensorDataset(X_cat_train, X_num_train, y_train)\n",
    "val_dataset = TensorDataset(X_cat_val, X_num_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "112bbafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Modified TabTransformer ===\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, categories, num_continuous, dim=32, depth=2, heads=4, mlp_hidden=64, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.cat_embeds = nn.ModuleList([\n",
    "            nn.Embedding(num_categories, dim) for num_categories in categories\n",
    "        ])\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, len(categories), dim))\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=heads, dropout=dropout, batch_first=True),\n",
    "            num_layers=depth\n",
    "        )\n",
    "\n",
    "        total_features = dim * len(categories) + num_continuous\n",
    "        self.norm = nn.LayerNorm(total_features)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(total_features, mlp_hidden),\n",
    "            nn.BatchNorm1d(mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_hidden, mlp_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(mlp_hidden + dim * len(categories) + num_continuous, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        embeds = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeds)]\n",
    "        x_cat = torch.stack(embeds, dim=1)              # (B, C, D)\n",
    "        x_cat = self.transformer(x_cat)                 # (B, C, D)\n",
    "        x_cat = x_cat.flatten(1)                        # (B, C*D)\n",
    "        x = torch.cat([x_cat, x_num], dim=1)            # (B, C*D + num_continuous)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        x_mlp = self.mlp(x)                             # (B, mlp_hidden)\n",
    "        out = self.head(torch.cat([x_mlp, x], dim=1))   # Concatenate for residual-style use\n",
    "        return out.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3bc1413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# === Init ===\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "categories = [df[col].nunique() for col in cat_cols]\n",
    "num_cont = len(num_cols)\n",
    "\n",
    "model = TabTransformer(categories, num_cont, dim=32, depth=2, heads=4, mlp_hidden=128, dropout=0.2).to(device)\n",
    "loss_fn = nn.SmoothL1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1aaaefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/100 | Train Loss: 1.0839 | MAE: 0.4387 | RMSE: 0.5527 | R²: 0.3556\n",
      "Epoch 002/100 | Train Loss: 0.1554 | MAE: 0.3433 | RMSE: 0.4529 | R²: 0.5673\n",
      "Epoch 003/100 | Train Loss: 0.1153 | MAE: 0.3312 | RMSE: 0.4245 | R²: 0.6199\n",
      "Epoch 004/100 | Train Loss: 0.1015 | MAE: 0.3179 | RMSE: 0.4167 | R²: 0.6338\n",
      "Epoch 005/100 | Train Loss: 0.0799 | MAE: 0.2804 | RMSE: 0.3708 | R²: 0.7100\n",
      "Epoch 006/100 | Train Loss: 0.0716 | MAE: 0.2428 | RMSE: 0.3412 | R²: 0.7544\n",
      "Epoch 007/100 | Train Loss: 0.0617 | MAE: 0.2957 | RMSE: 0.3810 | R²: 0.6938\n",
      "Epoch 008/100 | Train Loss: 0.0551 | MAE: 0.2524 | RMSE: 0.3533 | R²: 0.7368\n",
      "Epoch 009/100 | Train Loss: 0.0554 | MAE: 0.2212 | RMSE: 0.3203 | R²: 0.7837\n",
      "Epoch 010/100 | Train Loss: 0.0537 | MAE: 0.2364 | RMSE: 0.3243 | R²: 0.7782\n",
      "Epoch 011/100 | Train Loss: 0.0468 | MAE: 0.2466 | RMSE: 0.3526 | R²: 0.7378\n",
      "Epoch 012/100 | Train Loss: 0.0440 | MAE: 0.2145 | RMSE: 0.3222 | R²: 0.7811\n",
      "Epoch 013/100 | Train Loss: 0.0438 | MAE: 0.2092 | RMSE: 0.3221 | R²: 0.7811\n",
      "Epoch 014/100 | Train Loss: 0.0412 | MAE: 0.2089 | RMSE: 0.3204 | R²: 0.7835\n",
      "Epoch 015/100 | Train Loss: 0.0413 | MAE: 0.1971 | RMSE: 0.2991 | R²: 0.8113\n",
      "Epoch 016/100 | Train Loss: 0.0411 | MAE: 0.2208 | RMSE: 0.3282 | R²: 0.7728\n",
      "Epoch 017/100 | Train Loss: 0.0391 | MAE: 0.2535 | RMSE: 0.3508 | R²: 0.7404\n",
      "Epoch 018/100 | Train Loss: 0.0382 | MAE: 0.1949 | RMSE: 0.3000 | R²: 0.8102\n",
      "Epoch 019/100 | Train Loss: 0.0368 | MAE: 0.2014 | RMSE: 0.3048 | R²: 0.8041\n",
      "Epoch 020/100 | Train Loss: 0.0368 | MAE: 0.2004 | RMSE: 0.3028 | R²: 0.8066\n",
      "Epoch 021/100 | Train Loss: 0.0361 | MAE: 0.2632 | RMSE: 0.3623 | R²: 0.7231\n",
      "Epoch 022/100 | Train Loss: 0.0346 | MAE: 0.2311 | RMSE: 0.3347 | R²: 0.7638\n",
      "Epoch 023/100 | Train Loss: 0.0355 | MAE: 0.2328 | RMSE: 0.3310 | R²: 0.7690\n",
      "Epoch 024/100 | Train Loss: 0.0331 | MAE: 0.2612 | RMSE: 0.3621 | R²: 0.7235\n",
      "Epoch 025/100 | Train Loss: 0.0316 | MAE: 0.2178 | RMSE: 0.3146 | R²: 0.7913\n",
      "⏹️ Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "# === Train Loop ===\n",
    "best_r2 = -float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "best_model_path = '../models/best_tabtransformer.pt'\n",
    "\n",
    "epochs = 100\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x_cat, x_num, y in train_loader:\n",
    "        x_cat, x_num, y = x_cat.to(device), x_num.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_cat, x_num)\n",
    "        loss = loss_fn(preds, y.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_cat, x_num, y in val_loader:\n",
    "            x_cat, x_num = x_cat.to(device), x_num.to(device)\n",
    "            pred = model(x_cat, x_num).cpu().numpy()\n",
    "            val_preds.extend(pred)\n",
    "            val_targets.extend(y.numpy())\n",
    "\n",
    "    mae = mean_absolute_error(val_targets, val_preds)\n",
    "    rmse = mean_squared_error(val_targets, val_preds, squared=False)\n",
    "    r2 = r2_score(val_targets, val_preds)\n",
    "    val_losses.append(mae)\n",
    "    scheduler.step(mae)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03}/{epochs} | Train Loss: {avg_train_loss:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f} | R²: {r2:.4f}\")\n",
    "\n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        counter = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"\\u23f9\\ufe0f Early stopping triggered!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554617e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Final Metrics -> MAE: 107914.16, RMSE: 180664.27, R²: 0.7788\n"
     ]
    }
   ],
   "source": [
    "# === Final Evaluation ===\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "\n",
    "final_preds, final_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_cat, x_num, y in val_loader:\n",
    "        x_cat, x_num = x_cat.to(device), x_num.to(device)\n",
    "        pred = model(x_cat, x_num).cpu().numpy()\n",
    "        final_preds.extend(pred)\n",
    "        final_targets.extend(y.numpy())\n",
    "\n",
    "final_preds = np.expm1(np.array(final_preds).flatten())\n",
    "final_targets = np.expm1(np.array(final_targets).flatten())\n",
    "\n",
    "mae = mean_absolute_error(final_targets, final_preds)\n",
    "rmse = np.sqrt(mean_squared_error(final_targets, final_preds))\n",
    "r2 = r2_score(final_targets, final_preds)\n",
    "\n",
    "print(f\"\\n✅ Final Metrics -> MAE: {mae:.2f}, RMSE: {rmse:.2f}, R²: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
